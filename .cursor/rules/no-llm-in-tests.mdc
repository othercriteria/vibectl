---
description: Strictly prohibits unmocked LLM calls in test code
globs: ["tests/**/*.py"]
alwaysApply: true
---
# No LLM Calls in Tests

Mock all LLM calls in tests for speed and determinism.

<rule>
name: no_llm_in_tests
description: Prohibits unmocked LLM calls in test code
filters:
  # Match test files
  - type: file_pattern
    pattern: "^tests?/.*\\.(py|test\\.py|spec\\.py)$"
  # Match test-related code
  - type: content
    pattern: "(?i)\\b(test|spec|fixture|mock|patch)\\b"

actions:
  - type: reject
    conditions:
      # Direct LLM imports without mocking
      - pattern: "^\\s*import\\s+llm\\s*$"
        message: "Direct LLM imports in test files must be mocked"
      # Direct LLM usage without mocking
      - pattern: "(?<!mock_)(?<!patch\\(\")(?<!with patch\\(\")(?<!Mock\\()(?<!return_value = )\\b(llm\\.get_model|LLMHandler)\\b"
        message: "LLM calls in tests must be mocked"
      # Missing mock for LLM-related functions
      - pattern: "def\\s+test_.*(?!.*@patch.*llm)"
        message: "Test functions using LLM must mock all LLM calls"
      # Ensure token limit handling is mocked
      - pattern: "(?<!mock_model\\.prompt\\.side_effect = )Exception\\(.*token.*limit.*\\)"
        message: "Token limit handling must be properly mocked"

  - type: suggest
    message: |
      When writing tests with LLM:
      1. Mock all LLM calls: llm.get_model and LLM client objects
      2. Test happy path: mock_model.prompt.return_value = Mock(text=lambda: "response")
      3. Test error path: mock_model.prompt.side_effect = Exception("LLM error")
      4. Test token limits: mock token limit exceptions
      5. Verify mock calls: mock_model.prompt.assert_called_once_with(expected_prompt)
      6. Create fixtures for common mocks

examples:
  - input: |
      # Good: Mocked LLM usage with error handling
      @patch("llm.get_model")
      def test_get_command(mock_get_model):
          mock_model = Mock()
          mock_model.prompt.return_value = Mock(text=lambda: "test response")
          mock_get_model.return_value = mock_model

          result = handle_command("test")
          assert result == "test response"
    output: "Correctly mocked LLM calls with error handling"

metadata:
  priority: critical  # Upgraded from high due to performance impact
  version: 1.2  # Updated version for removing project-specific references
</rule>
