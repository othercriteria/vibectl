# Planned Changes for LLM Proxy Server Feature

## Overview

Implement a client/server architecture that allows vibectl CLI instances to delegate LLM calls to a centralized vibectl server. This decouples clients from needing to manage model choice, provider authentication, and API key management.

## Motivation

- **Simplified client setup**: Users don't need to configure API keys or choose models
- **Centralized management**: Server handles model selection, rate limiting, cost control
- **Enterprise scenarios**: Central LLM proxy with usage controls and observability
- **Consistent behavior**: All clients use the same model configuration

## Architecture

### Client/Server Split

- **Client**: Delegates LLM calls to server, receives responses
- **Server**: Manages models, API keys, usage limits, and actual LLM execution
- **Protocol**: gRPC for strong typing, streaming support, and k8s ecosystem fit

### ‚úÖ Configuration Structure (COMPLETED)

**Status**: Implemented and deployed locally

Move from flat config to structured approach:

### Current Flat Structure Issues

The current config structure is a flat dictionary with 30+ top-level keys:
- No logical grouping of related settings
- Verbose key names like `live_display_max_lines`, `live_display_wrap_text`
- Difficult to navigate and understand relationships
- Hard to extend for client/server separation

### ‚úÖ New Hierarchical Structure (COMPLETED)

Reorganized config into logical sections and implemented directory structure:

**Configuration Location**: `~/.config/vibectl/client/config.yaml` (standardized on XDG Base Directory specification)

```yaml
# ~/.config/vibectl/client/config.yaml
core:
  kubeconfig: /etc/rancher/k3s/k3s.yaml
  kubectl_command: kubectl

display:
  theme: light
  show_raw_output: false
  show_vibe: true
  show_kubectl: true
  show_memory: true
  show_iterations: true
  show_metrics: all
  colored_output: true
  show_streaming: true

llm:
  model: claude-4-sonnet
  max_retries: 2
  retry_delay_seconds: 1.0

providers:
  anthropic:
    key: null
    key_file: null
  ollama:
    key: null
    key_file: null
  openai:
    key: null
    key_file: null

memory:
  enabled: true
  max_chars: 1000

warnings:
  warn_no_output: true
  warn_no_proxy: true

live_display:
  max_lines: 10
  wrap_text: false
  stream_buffer_max_lines: 100000
  default_filter_regex: null
  save_dir: .

features:
  intelligent_apply: true
  intelligent_edit: true
  max_correction_retries: 1
  check_max_iterations: 10

networking:
  intermediate_port_range: 10000-11000

plugins:
  precedence:
    - paranoid-security-vibe
    - terse-minimalist-vibe
    - devious-organizer-vibe
    - clumsy-vibe
    - focused-recovery-suggestions
    - smart-port-selection
    - memory-update-counter-v1
    - intelligent-edit-enhanced
    - security-focused-check
    - apply-comprehensive-demo
    - annotating-patch

system:
  log_level: WARNING
  custom_instructions: Use a ton of emojis! üòÅ

# Auto-managed by vibectl memory commands
memory_content: |
  User working on Kubernetes deployment testing across multiple namespaces...
```

### ‚úÖ Enhanced CLI Features (COMPLETED)

1. **Sectioned Display**:
   ```bash
   vibectl config show           # Show all sections with headers
   vibectl config show llm       # Show only LLM section
   vibectl config show display   # Show only display section
   ```

2. **Dotted Path Access**:
   ```bash
   vibectl config set llm.model claude-4-sonnet
   vibectl config set display.theme dark
   vibectl config unset memory.max_chars  # Reset to default
   vibectl config show llm.max_retries    # Show single value
   ```

3. **Config File Location Display**:
   ```bash
   vibectl config info
   # Output:
   # Configuration file: ~/.config/vibectl/client/config.yaml
   # Type: client
   # Last modified: 2024-12-30 15:30:45
   ```

4. **Section Validation**:
   - Validate section names and keys during set operations
   - Show helpful error messages for invalid paths
   - Support tab completion for available sections/keys

### ‚úÖ Client/Server Config Separation (Directory Structure Ready)

Directory structure implemented for client/server split:

```
~/.config/vibectl/
‚îú‚îÄ‚îÄ client/
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml          # client-specific config (above structure)
‚îÇ   ‚îî‚îÄ‚îÄ server-secrets/      # server connection secrets (for future)
‚îÇ       ‚îî‚îÄ‚îÄ prod-server.key
‚îú‚îÄ‚îÄ plugins/                 # shared plugin storage
‚îÇ   ‚îú‚îÄ‚îÄ apply-comprehensive-demo-1.0.0.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ server/                  # server config (for future implementation)
    ‚îú‚îÄ‚îÄ config.yaml          # server config (models, limits, etc.)
    ‚îî‚îÄ‚îÄ client-secrets/      # issued client secrets
        ‚îú‚îÄ‚îÄ user1.key
        ‚îî‚îÄ‚îÄ user2.key
```

**Changes Made**:
- ‚úÖ Updated `Config` class to use `~/.config/vibectl/client/config.yaml`
- ‚úÖ Moved existing config from `~/.vibectl/config.yaml` to new location
- ‚úÖ Updated plugin storage to `~/.config/vibectl/plugins/` (shared across client/server)
- ‚úÖ Verified backward compatibility removal works correctly
- ‚úÖ Updated all test files to use new directory structure

## Model Adapter Architecture

Implement `ProxyModelAdapter` that:
- Implements existing `ModelAdapter` interface
- Forwards `execute()` and `execute_and_log_metrics()` calls to server via gRPC
- Maintains same error handling and metrics collection patterns
- Coexists with direct LLM usage (no fallback - explicit choice)

## Protocol Design

### gRPC Service Definition

```protobuf
service VibectlLLMProxy {
  rpc Execute(ExecuteRequest) returns (ExecuteResponse);
  rpc StreamExecute(ExecuteRequest) returns (stream StreamChunk);
  rpc GetServerInfo(GetServerInfoRequest) returns (GetServerInfoResponse);
}
```

### Key Message Types

- `ExecuteRequest`: system_fragments, user_fragments, model_name (optional), response_model_schema
- `ExecuteResponse`: success/error union with response_text, metrics, actual_model_used
- `GetServerInfoResponse`: available_models, default_model, server limits

### Authentication

- **Shared secrets** with expiration times (months/years for typical usage)
- **URL format**: `vibectl-server://secret@llm-server.company.com:443`
- **JWT tokens** for transparent secret passing
- **Out-of-band distribution** initially (copy-paste friendly)

## Implementation Plan

### Phase 1: Core Infrastructure

1. **gRPC Protocol Definition**
   - Define .proto files for service interface
   - Generate Python gRPC stubs
   - Add grpcio dependencies to pyproject.toml

2. **Configuration Refactoring**
   - Restructure config from flat to hierarchical
   - Implement client/server config separation
   - Maintain backward compatibility for existing configs

3. **ProxyModelAdapter Implementation**
   - Implement ModelAdapter interface for proxy calls
   - Handle gRPC client lifecycle and connection management
   - Error mapping from gRPC errors to existing error types

### Phase 2: Server Implementation

4. **Server Core**
   - gRPC server with ExecuteRequest handling
   - Integration with existing LLMModelAdapter for actual LLM calls
   - Secret validation and client authentication

5. **Client Management**
   - Secret generation and distribution mechanism
   - Client authentication and authorization
   - Basic rate limiting and usage tracking

### Phase 3: Client Integration

6. **Client Detection and Configuration**
   - Auto-detection of proxy vs direct mode based on config
   - `vibectl setup-proxy` command for connection setup
   - Validation and testing of proxy connections

7. **Model Selection Logic**
   - Decouple proxy and direct LLM model selection
   - Server-side model selection and client model requests
   - Clear precedence rules for model selection

### Phase 4: Advanced Features

8. **Streaming Support**
   - Implement StreamExecute for streaming responses
   - Maintain compatibility with existing streaming interfaces

9. **Server Features**
   - Multiple model support and routing
   - Advanced rate limiting and usage controls
   - Observability and metrics collection

10. **Setup and Tooling**
    - Enhanced setup commands for server deployment
    - Connection testing and diagnostic tools
    - Documentation and examples

## Files to be Created/Modified

### New Files
- `vibectl/proto/llm_proxy.proto` - gRPC service definition
- `vibectl/proto/llm_proxy_pb2.py` - Generated protobuf classes
- `vibectl/proto/llm_proxy_pb2_grpc.py` - Generated gRPC stubs
- `vibectl/proxy_model_adapter.py` - ProxyModelAdapter implementation
- `vibectl/server/` - Server implementation directory
- `vibectl/server/main.py` - Server entry point
- `vibectl/server/auth.py` - Authentication and secret management
- `vibectl/subcommands/setup_proxy.py` - Setup command implementation

### Modified Files
- `vibectl/config.py` - Restructure from flat to hierarchical config
- `vibectl/model_adapter.py` - Integration points for proxy adapter
- `vibectl/cli.py` - Add setup-proxy command
- `pyproject.toml` - Add gRPC dependencies
- `STRUCTURE.md` - Document new architecture

## Testing Strategy

- **Unit tests**: ProxyModelAdapter with mocked gRPC calls
- **Integration tests**: End-to-end client/server communication
- **Configuration tests**: Config migration and validation
- **Error handling tests**: Network failures, authentication errors
- **Performance tests**: Latency comparison with direct LLM calls

## Migration Strategy

- **Backward compatibility**: Existing direct LLM usage continues unchanged
- **Opt-in**: Proxy mode enabled only when explicitly configured
- **Gradual adoption**: Users can migrate individual commands/workflows
- **Clear errors**: Helpful messages when proxy is misconfigured

## Success Criteria

1. **Functional**: Client can successfully delegate LLM calls to server
2. **Transparent**: Same user experience as direct LLM calls
3. **Performant**: Minimal latency overhead from proxying
4. **Secure**: Proper authentication and secret management
5. **Maintainable**: Clean separation of concerns and testable code
6. **Documented**: Clear setup and usage documentation

## Open Questions

1. **Connection lifecycle**: Persistent connections vs per-request?
2. **Error retry logic**: How should transient network errors be handled?
3. **Server discovery**: Auto-discovery vs explicit configuration?
4. **Model caching**: Should server cache model instances?
5. **Metrics aggregation**: How to aggregate metrics across multiple clients?

## Dependencies

- `grpcio` and `grpcio-tools` for gRPC support
- Potential JWT library for token handling
- Configuration migration utilities
- Enhanced testing infrastructure for client/server scenarios

## ‚úÖ Provider Configuration Restructuring (COMPLETED)

**Status**: Implemented as part of hierarchical config restructuring

### Solution Implemented

Restructured provider configuration into explicit hierarchy:

**From (old flat structure):**
```yaml
llm:
  model_keys:
    openai: "sk-..."
    anthropic: "sk-ant-..."
  model_key_files:
    openai: "/path/to/key"
    anthropic: "/path/to/key"
```

**To (new hierarchical structure):**
```yaml
providers:
  openai:
    key: "sk-..."
    key_file: "/path/to/key"
  anthropic:
    key: "sk-ant-..."
    key_file: "/path/to/key"
  ollama:
    key: null
    key_file: null
```

### Benefits Achieved

1. **Cleaner paths**: `providers.openai.key` vs `llm.model_keys.openai`
2. **Explicit schema**: No arbitrary dictionary keys, all paths defined explicitly
3. **Simpler validation**: Standard hierarchical validation, no special cases
4. **Better organization**: Provider-specific config grouped together
5. **Extensible**: Easy to add new providers or provider-specific settings

### Implementation Completed

1. ‚úÖ **Updated DEFAULT_CONFIG**: Moved provider keys to new `providers` section
2. ‚úÖ **Updated CONFIG_SCHEMA**: Defined explicit provider structure
3. ‚úÖ **Updated validation**: Removed special dictionary key handling
4. ‚úÖ **Updated config methods**: Modified `get_model_key` and `set_model_key` methods
5. ‚úÖ **Updated tests**: Changed all `llm.model_keys.*` paths to `providers.*.key`
6. ‚úÖ **Updated CLI examples**: Updated help text and documentation
